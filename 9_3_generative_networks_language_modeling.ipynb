{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyziPmrubsRFkM58SGt2z4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhiosutoyo/BERT-Research/blob/main/9_3_generative_networks_language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative Networks: Language Modeling\n",
        "\n",
        "This Python code trains a word-level language model using an LSTM in PyTorch. It begins by downloading and preprocessing a text dataset, converting words to integers for model training. The `TextDataset` class creates input-target pairs for the model. In this example, we utilize Cinderella Story for the dataset.\n",
        "\n",
        "Then, the LSTM model is defined with embedding, LSTM, and fully connected layers. The training loop optimizes the model over multiple epochs using cross-entropy loss. A text generation function is provided, ensuring the starting words are in the vocauary, and generating new sequences based on the trained model.\n",
        "\n",
        "Lastly, the vocabulary is printed for verification."
      ],
      "metadata": {
        "id": "IST67WMASoEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "\n",
        "# Download the text data\n",
        "url = \"https://raw.githubusercontent.com/rhiosutoyo/Teaching-Deep-Learning-and-Its-Applications/main/dataset/cinderella-story.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Preprocessing\n",
        "words = text.split()\n",
        "vocab = Counter(words)\n",
        "vocab = sorted(vocab, key=vocab.get, reverse=True)\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "encoded_text = [word_to_idx[word] for word in words]\n",
        "\n",
        "# Parameters\n",
        "sequence_length = 4\n",
        "batch_size = 2\n",
        "\n",
        "# Prepare the dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, sequence_length):\n",
        "        self.text = text\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.tensor(self.text[idx:idx + self.sequence_length]),\n",
        "            torch.tensor(self.text[idx + 1:idx + self.sequence_length + 1]),\n",
        "        )\n",
        "\n",
        "dataset = TextDataset(encoded_text, sequence_length)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the LSTM-based language model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new_zeros(num_layers, batch_size, hidden_dim),\n",
        "                weight.new_zeros(num_layers, batch_size, hidden_dim))\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 10\n",
        "hidden_dim = 50\n",
        "num_layers = 2\n",
        "\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        # Initialize hidden state with batch size\n",
        "        hidden = model.init_hidden(inputs.size(0))\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Detach hidden state to prevent backpropagating through the entire training history\n",
        "        hidden = tuple([each.detach() for each in hidden])\n",
        "\n",
        "        # Forward pass\n",
        "        outputs, hidden = model(inputs, hidden)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdR_OhvqPyUr",
        "outputId": "097469fc-4d4e-47cd-dc7a-af973d032f87"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/200], Loss: 3.4668\n",
            "Epoch [10/200], Loss: 1.3209\n",
            "Epoch [20/200], Loss: 0.5407\n",
            "Epoch [30/200], Loss: 1.4255\n",
            "Epoch [40/200], Loss: 0.4448\n",
            "Epoch [50/200], Loss: 0.6190\n",
            "Epoch [60/200], Loss: 0.4712\n",
            "Epoch [70/200], Loss: 0.6780\n",
            "Epoch [80/200], Loss: 2.1376\n",
            "Epoch [90/200], Loss: 0.0535\n",
            "Epoch [100/200], Loss: 0.9925\n",
            "Epoch [110/200], Loss: 1.2489\n",
            "Epoch [120/200], Loss: 0.6730\n",
            "Epoch [130/200], Loss: 0.0051\n",
            "Epoch [140/200], Loss: 0.7394\n",
            "Epoch [150/200], Loss: 0.9580\n",
            "Epoch [160/200], Loss: 0.0171\n",
            "Epoch [170/200], Loss: 0.0275\n",
            "Epoch [180/200], Loss: 0.9480\n",
            "Epoch [190/200], Loss: 1.0516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generation function\n",
        "def generate_text(model, start_text, length=20):\n",
        "    model.eval()\n",
        "    words = start_text.split()\n",
        "    state_h, state_c = model.init_hidden(1)\n",
        "\n",
        "    for _ in range(length):\n",
        "        x = torch.tensor([[word_to_idx[w] for w in words[-sequence_length:]]])\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
        "        word_idx = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(idx_to_word[word_idx])\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Generate text with a default start text that is in the vocabulary\n",
        "start_text = \"Cinderella was\"\n",
        "print(generate_text(model, start_text))\n",
        "\n",
        "start_text = \"Cinderella was\"\n",
        "print(generate_text(model, start_text))\n",
        "\n",
        "start_text = \"Cinderella was\"\n",
        "print(generate_text(model, start_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXN2oUIBR4kR",
        "outputId": "5e0e0e6e-77bf-4165-b1c0-ee32530e6cbb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cinderella was left alone in her old unfortunately, sparkled bells terribly twelve, the piece of advice: \"Remember, my dear, the magic she\n",
            "Cinderella was the most important thing in style. the maiden the palace, and every maiden in the land was invited. Cinderella's the\n",
            "Cinderella was the happiest she had ever been. But and The stepmother had heard about in her stepmother's young girl named Cinderella.\n"
          ]
        }
      ]
    }
  ]
}